# -*- coding: utf-8 -*-
"""image_caption.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YzKwGXnBtskF4lSDXIBfJll05sncssTh
"""

!pip install -q transformers accelerate torch torchvision pillow gradio


import torch
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    AutoTokenizer,
    AutoModelForCausalLM,
)
import gradio as gr
from PIL import Image
import numpy as np


device = "cuda" if torch.cuda.is_available() else "cpu"


caption_model_id = "Salesforce/blip-image-captioning-base"
vqa_model_id = "Salesforce/blip-vqa-base"  # for simple visual Q&A

caption_processor = BlipProcessor.from_pretrained(caption_model_id)
caption_model = BlipForConditionalGeneration.from_pretrained(caption_model_id).to(device)

vqa_processor = BlipProcessor.from_pretrained(vqa_model_id)
vqa_model = BlipForConditionalGeneration.from_pretrained(vqa_model_id).to(device)


llm_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # lightweight, ungated

llm_tokenizer = AutoTokenizer.from_pretrained(llm_id)
llm_model = AutoModelForCausalLM.from_pretrained(
    llm_id,
    device_map="auto",
    torch_dtype=torch.float16,
)




def preprocess_pil(image):
    """Ensure we always work with a reasonably sized PIL image."""
    if image is None:
        return None

    if isinstance(image, np.ndarray):
        image = Image.fromarray(image.astype("uint8"))

    if not isinstance(image, Image.Image):
        image = Image.open(image).convert("RGB")
    else:
        image = image.convert("RGB")

    # Downscale very large images to speed up inference
    max_side = 512
    w, h = image.size
    if max(w, h) > max_side:
        scale = max_side / float(max(w, h))
        image = image.resize((int(w * scale), int(h * scale)))

    return image


def get_blip_caption(image, max_length=30, num_beams=5):
    """Generate a short caption from BLIP captioning model."""
    inputs = caption_processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        out = caption_model.generate(
            **inputs,
            max_length=max_length,
            num_beams=num_beams,
        )
    caption = caption_processor.decode(out[0], skip_special_tokens=True).strip()
    return caption


def get_blip_vqa_answer(image, question, max_length=30, num_beams=5):
    """Answer a simple question about the image using BLIP VQA model."""
    prompt = f"Question: {question} Answer:"
    inputs = vqa_processor(images=image, text=prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        out = vqa_model.generate(
            **inputs,
            max_length=max_length,
            num_beams=num_beams,
        )
    text = vqa_processor.decode(out[0], skip_special_tokens=True).strip()

    # BLIP often returns a full "Question: ... Answer: ..." sequence; keep only answer part
    if "Answer:" in text:
        text = text.split("Answer:")[-1].strip()
    return text


def explain_with_llm(caption, qa_answer=None, user_question=""):
    """Use the small instruction-style LLM to produce a detailed explanation of the image."""
    prompt = (
        "You are an expert visual tutor. You are given information about an image.\n"
        "Use this to explain what is happening in the image in 3â€“5 simple sentences.\n\n"
        f"Caption: {caption}\n"
    )

    if qa_answer:
        prompt += f"Short answer to user question: {qa_answer}\n"
    if user_question and user_question.strip():
        prompt += f"User question: {user_question.strip()}\n"

    prompt += "\nNow give a clear explanation for a non-expert:\n"

    inputs = llm_tokenizer(prompt, return_tensors="pt").to(llm_model.device)
    with torch.no_grad():
        outputs = llm_model.generate(
            **inputs,
            max_new_tokens=220,
            do_sample=True,
            top_p=0.9,
            temperature=0.7,
        )
    text = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Strip off the prompt if it repeats
    if "Now give a clear explanation for a non-expert:" in text:
        text = text.split("Now give a clear explanation for a non-expert:")[-1]

    return text.strip()



def analyze_image(image, question):
    """
    Gradio callback:
    - image: uploaded image
    - question: optional question about the image
    Returns: caption, short answer, detailed explanation
    """
    if image is None:
        return "Please upload an image.", "", ""

    img = preprocess_pil(image)

    # 1) Caption from BLIP
    try:
        caption = get_blip_caption(img)
    except Exception as e:
        caption = f"Error generating caption: {e}"

    # 2) Short answer from BLIP VQA (if question provided)
    question = question or ""
    qa_answer = ""
    if question.strip():
        try:
            qa_answer = get_blip_vqa_answer(img, question.strip())
        except Exception as e:
            qa_answer = f"Error generating answer: {e}"
    else:
        qa_answer = "No specific question asked."

    # 3) Detailed explanation from LLM
    try:
        explanation = explain_with_llm(
            caption=caption,
            qa_answer=None if "Error" in qa_answer else (qa_answer if question.strip() else None),
            user_question=question,
        )
    except Exception as e:
        explanation = f"Error generating explanation: {e}"

    return caption, qa_answer, explanation




custom_css = """
body { background-color: #050816; }
.gradio-container { max-width: 1100px !important; margin: auto; }
#title-bar {
  font-size: 26px;
  font-weight: 700;
  padding: 16px 20px;
  margin-bottom: 8px;
  border-radius: 16px;
  background: radial-gradient(circle at top left, #22c1c3, #0f172a 55%, #111827);
  color: white;
}
#subtitle {
  font-size: 14px;
  opacity: 0.85;
}
.card {
  border-radius: 16px !important;
  border: 1px solid rgba(148, 163, 184, 0.4) !important;
  background: rgba(15, 23, 42, 0.95) !important;
}
label { color: #e5e7eb !important; }
"""

with gr.Blocks(css=custom_css, theme="soft") as demo:
    # Header
    with gr.Row():
        with gr.Column():
            gr.HTML(
                f"""
                <div id="title-bar">
                  BLIP Vision Explanation Studio
                  <div id="subtitle">
                    Upload an image, optionally ask a question, and get a caption, a short answer, and a detailed explanation.
                    <br>Running on: <b>{device.upper()}</b>
                  </div>
                </div>
                """
            )

    with gr.Row():
        # Left: inputs
        with gr.Column(scale=1, elem_classes=["card"]):
            image_input = gr.Image(
                label="Upload an image",
                type="pil",
                height=300,
            )
            question_input = gr.Textbox(
                label="Optional question about the image",
                placeholder="e.g., What is the person doing? Is this indoors or outdoors?",
            )
            analyze_button = gr.Button("Analyze Image", variant="primary")

        # Right: outputs
        with gr.Column(scale=1, elem_classes=["card"]):
            caption_output = gr.Textbox(
                label="BLIP Caption",
                lines=2,
                interactive=False,
            )
            answer_output = gr.Textbox(
                label="BLIP Short Answer",
                lines=2,
                interactive=False,
            )
            explanation_output = gr.Textbox(
                label="LLM Detailed Explanation",
                lines=8,
                interactive=False,
            )

    analyze_button.click(
        fn=analyze_image,
        inputs=[image_input, question_input],
        outputs=[caption_output, answer_output, explanation_output],
    )


demo.launch(share=True)